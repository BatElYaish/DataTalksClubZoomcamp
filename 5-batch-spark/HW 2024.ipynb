{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark master\n",
    "spark_master = \"local[*]\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "            .master(spark_master) \\\n",
    "            .appName(\"HW 2024\")\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>HW 2024</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x264ae6daf60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File location https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz\n",
    "file_path = os.getcwd()+\"/data/Raw/2019/fhv_tripdata_2019-10.csv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = spark.read.format('csv')\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"delimiter\",',')\\\n",
    "            .load(file_path)\\\n",
    "            .limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dispatching_base_num',\n",
       " 'pickup_datetime',\n",
       " 'dropOff_datetime',\n",
       " 'PUlocationID',\n",
       " 'DOlocationID',\n",
       " 'SR_Flag',\n",
       " 'Affiliated_base_number']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|         264|         264|   NULL|                B00009|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|         264|         264|   NULL|                B00013|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|         264|         264|   NULL|                B00014|\n",
      "|              B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|         264|         264|   NULL|                B00014|\n",
      "|              B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|         264|         264|   NULL|                B00014|\n",
      "|     B00021         |2019-10-01 00:00:48|2019-10-01 00:07:12|         129|         129|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:47:23|2019-10-01 00:53:25|          57|          57|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:10:06|2019-10-01 00:19:50|         173|         173|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:51:37|2019-10-01 01:06:14|         226|         226|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:28:23|2019-10-01 00:34:33|          56|          56|   NULL|       B00021         |\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropOff_datetime: string (nullable = true)\n",
      " |-- PUlocationID: string (nullable = true)\n",
      " |-- DOlocationID: string (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To get the schema\n",
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert schema to types\n",
    "schema = t.StructType([\n",
    "t.StructField(\"dispatching_base_num\",t.StringType()),\n",
    "t.StructField(\"pickup_datetime\",t.TimestampType()),\n",
    "t.StructField(\"dropOff_datetime\",t.TimestampType()),\n",
    "t.StructField(\"PUlocationID\",t.IntegerType()),\n",
    "t.StructField(\"DOlocationID\",t.IntegerType()),\n",
    "t.StructField(\"SR_Flag\",t.StringType()),\n",
    "t.StructField(\"Affiliated_base_number\",t.StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV with updated schema\n",
    "df_csv = spark.read.format('csv')\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"delimiter\",',')\\\n",
    "            .schema(schema)\\\n",
    "            .load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now we have the new schema\n",
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.getcwd()+\"/data/Processed/FHV/2019/10/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repartition the Dataframe to 6 partitions and save it to parquet.\n",
    "df_csv.repartition(6)\\\n",
    "    .write.parquet(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.366528511047363"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get file size\n",
    "os.path.getsize(folder_path+[f for f in os.listdir(folder_path) if f.endswith(\".parquet\")][0])/ (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read parquet df\n",
    "\n",
    "df = spark.read.parquet(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00254|2019-10-24 21:26:26|2019-10-24 22:20:05|         237|         265|   NULL|                B00254|\n",
      "|              B02795|2019-10-23 14:09:00|2019-10-23 14:57:13|         238|         183|   NULL|                B02795|\n",
      "|              B00248|2019-10-07 23:51:13|2019-10-08 00:11:55|         264|          16|   NULL|                B00248|\n",
      "|              B01087|2019-10-11 01:45:09|2019-10-11 02:17:12|          50|          35|   NULL|                B01087|\n",
      "|              B03016|2019-10-29 21:14:33|2019-10-29 21:25:46|         264|          32|   NULL|                B02864|\n",
      "|              B00887|2019-10-21 14:12:30|2019-10-21 14:48:15|         264|         265|   NULL|                B02465|\n",
      "|              B01061|2019-10-24 08:00:55|2019-10-24 08:23:53|         264|         240|   NULL|                B01061|\n",
      "|              B01506|2019-10-24 17:02:32|2019-10-24 17:10:15|         179|           7|   NULL|                B01506|\n",
      "|              B01231|2019-10-16 17:03:53|2019-10-16 17:10:50|         264|         217|   NULL|                B01231|\n",
      "|              B02293|2019-10-30 14:36:17|2019-10-30 15:13:51|          33|          29|   NULL|                B02293|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|count_rows|\n",
      "+----------+\n",
      "|     62610|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#How many taxi trips were there that started on the 15th of October?\n",
    "df.filter(f.to_date(f.col('pickup_datetime')) == '2019-10-15')\\\n",
    "    .select(f.count('*').alias('count_rows')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|longest_trip|\n",
      "+------------+\n",
      "|   6311525.0|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What is the length of the longest trip in the dataset in hours?\n",
    "\n",
    "df.withColumn('Trip_length', f.col('dropOff_datetime').cast('long')-f.col('pickup_datetime').cast('long'))\\\n",
    ".select((f.max('Trip_length')/360).alias('longest_trip')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|       Zone|\n",
      "+-----------+\n",
      "|Jamaica Bay|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# what is the name of the LEAST frequent pickup location Zone?\n",
    "df_zones = spark.read.format('csv')\\\n",
    "        .option('header','true')\\\n",
    "        .load(os.getcwd()+\"/data/taxi_zone_lookup.csv\")\\\n",
    "        .select(f.col('LocationID').alias('PUlocationID'),'Zone')\n",
    "\n",
    "df.join(df_zones,'PUlocationID','inner')\\\n",
    "    .groupBy('Zone')\\\n",
    "    .agg(f.count('*').alias('cnt'))\\\n",
    "    .orderBy(f.col('cnt'))\\\n",
    "    .select('Zone').limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if spark is local\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
